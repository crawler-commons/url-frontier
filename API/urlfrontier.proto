/**
 * Licensed to Crawler-Commons under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";

option java_package = "crawlercommons.urlfrontier";

package urlfrontier;

service URLFrontier {
     /** Return the names of up to N active queues
      a queue is active if it has URLs due for fetching; by default the service will return up to 100 results from offset 0 **/
     rpc ListQueues(Pagination) returns (QueueList) {}

     /** Stream URLs due for fetching from M queues with up to N items per queue **/
     rpc GetURLs(GetParams) returns (stream URLInfo) {}

     /** Push URL items to the server; they get created (if they don't already exist) in case of DiscoveredURLItems or updated if KnownURLItems **/
     rpc PutURLs(stream URLItem) returns (stream String) {}

     /** Return stats for a specific queue or the whole crawl if the value if empty or null **/
     rpc GetStats(String) returns (Stats) {}
     
     /** Delete the queue based on the key in parameter, returns the number of URLs removed this way **/
     rpc DeleteQueue(String) returns (Integer) {}

     /** Block a queue from sending URLs; the argument is the number of seconds of UTC time since Unix epoch
      1970-01-01T00:00:00Z. The default value of 0 will unblock the queue. The block will get removed once the time
      indicated in argument is reached. This is useful for cases where a server returns a Retry-After for instance. 
      **/
     rpc BlockQueueUntil(BlockQueueParams) returns (Empty) {}
     
     /** De/activate the crawl. GetURLs will not return anything until SetActive is set to true. PutURLs will still take incoming data. **/
     rpc SetActive(Boolean) returns (Empty) {}

     /** Returns true if the crawl is active, false if it has been deactivated with SetActive(Boolean) **/
     rpc GetActive(Empty) returns (Boolean) {}
     
     /** Set a delay from a given queue.
      No URLs will be obtained via GetURLs for this queue until the number of seconds specified has 
      elapsed since the last time URLs were retrieved.
       Usually informed by the delay setting of robots.txt.
      **/
     rpc SetDelay(QueueDelayParams) returns (Empty) {}
}

/** 
* Message returned by the GetStats method
**/
message Stats {
  // number of active URLs in queues
  uint64 size = 1;
  // number of URLs currently in flight
  uint32 inProcess = 2;
  // custom counts 
  map<string, uint64> counts = 3;
  // number of active queues in the frontier
  uint64 numberOfQueues = 4;
}

message Pagination {
  // position of the first result in the list; defaults to 0
  uint32 start = 1;
  // max number of values; defaults to 100
  uint32 size = 2;
}

message Empty {
}

message Boolean {
  bool state = 1;
}

message String {
  string value = 1;
}

message Integer {
  uint64 value = 1;
}

/** Returned by ListQueues **/
message QueueList {
  repeated string values = 1;
  // total number of queues
  uint64 total = 2;
  // position of the first result in the list
  uint32 start = 3;
  // number of values returned
  uint32 size = 4;
}

message StringList {
  repeated string values = 1;
}


/** Parameter message for SetDelay **/
message QueueDelayParams {
  /** ID for the queue - an empty value sets the default for all the queues **/
  string key = 1;
  //  delay in seconds before a queue can provide new URLs
  uint32 delay_requestable = 2;
}

/** Parameter message for BlockQueueUntil **/
message BlockQueueParams {
  /** ID for the queue **/
  string key = 1;
  /** Expressed in seconds of UTC time since Unix epoch
      1970-01-01T00:00:00Z. The default value of 0 will unblock the queue.
  **/
  uint64 time = 2;
}

/** Parameter message for GetURLs **/
message GetParams {
  // maximum number of URLs per queue, the default value of 0 means no limit
  uint32 max_urls_per_queue = 1;
  // maximum number of queues to get URLs from, the default value of 0 means no limit
  uint32 max_queues = 2;
  // queue id if restricting to a specific queue
  string key = 3;
  //  delay in seconds before a URL can be unlocked and sent again for fetching 
  uint32 delay_requestable = 4;
}

/** Wrapper for a KnownURLItem or DiscoveredURLItem **/
message URLItem {
oneof item {
    DiscoveredURLItem discovered = 1;
    KnownURLItem known = 2;
  }
}

message URLInfo {
  /** URL **/
  string url = 1;
  /** The key is used to put the URLs into queues, the value can be anything set by the client but would typically be the hostname,
    domain name or IP or the URL. If not set, the service will use a sensible default like hostname.
  **/
  string key = 2;
  /** 
  Arbitrary key / values stored alongside the URL. Can be anything needed by the crawler like http status, source URL etc...
  **/
  map<string, StringList> metadata = 3;
}

/**
 URL which was already known in the frontier, was returned by GetURLs() and processed by the crawler. Used for updating the information 
 about it in the frontier. If the date is not set, the URL will be considered done and won't be resubmitted for fetching, otherwise
 it will be elligible for fetching after the delay has elapsed.
**/
message KnownURLItem {
  URLInfo info = 1;
  /** Expressed in seconds of UTC time since Unix epoch
      1970-01-01T00:00:00Z. Optional, the default value of 0 indicates
      that a URL should not be refetched.
  **/
  uint64 refetchable_from_date = 2;
}

/**
 URL discovered during the crawl, might already be known in the URL Frontier or not.
**/
message DiscoveredURLItem {
  URLInfo info = 1;
}


